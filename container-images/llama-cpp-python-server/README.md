# llama-cpp server

This image runs an OpenAI compatible llama server.

It expects an gguf model available at a location specified through the
environment variable `MODEL`.

For example usage, look at [llama.container](../../.config/containers/systemd/llama.container)
